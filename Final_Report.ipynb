{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkhG--sMxGz9"
      },
      "source": [
        "# **Automatic Speech Recognition with Transformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUxXUOkaxN5J"
      },
      "source": [
        "##**Introduction**\n",
        "\n",
        "\n",
        "- **Đề tài:** *Automatic Speech Recognition with Transformer* (Nhận dạng giọng nói tự động với Transformer)\n",
        "- **Dữ liệu:** Gồm các file âm thanh được thu thập từ trang VOA: https://learningenglish.voanews.com \n",
        "Chuyển file âm thanh **.mp3 64kbps** sang âm thanh **Sample rate: 16k Hz, 1 Channel, wav**\n",
        "\n",
        "    1.   Chia tất cả các file âm thanh thành câu. Đặt vào folder và bắt đầu từ số thứ tự là 1. Ví dụ: \n",
        "    Folder: African_Dinosaur_Discovered_in_Morocco\n",
        "\n",
        "    /1.wav\n",
        "    \n",
        "    /2.wav\n",
        "\n",
        "    /... \n",
        "\n",
        "    2.   Chia tất cả đoạn văn thành từng câu và xuống dòng. Đặt vào folder Story. Ví dụ: \n",
        "\n",
        "    Folder: African_Dinosaur_Discovered_in_Morocco\n",
        "\n",
        "    /African_Dinosaur_Discovered_in_Morocco.txt\n",
        "    3.   Mapping âm thanh và text với nhau.\n",
        "\n",
        "- **Chia dữ liệu để Huấn luyện mô hình:**\n",
        "    1.   Training Data: 80%\n",
        "    2.   Evaluation Data: 20%\n",
        "\n",
        "- **Mô hình:** Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx5NY-2oxhzE"
      },
      "source": [
        "## **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNmAnxuV-kKW",
        "outputId": "f686bfc1-e13c-4b0a-cd38-ed3ae212889a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.7/dist-packages (2.3.0)\n",
            "Requirement already satisfied: python-Levenshtein==0.12.2 in /usr/local/lib/python3.7/dist-packages (from jiwer) (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein==0.12.2->jiwer) (57.4.0)\n"
          ]
        }
      ],
      "source": [
        "#Thư viện tính Word Error Rate\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "5dY0ROidCFx1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import natsort\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "from librosa.feature import mfcc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from jiwer import wer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWH6oW8GzNH-"
      },
      "source": [
        "### **Download the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdRXQpMMycGP"
      },
      "source": [
        "**Get link share từ Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuYoX01PAw7F",
        "outputId": "391e21e8-b3b3-4868-d21b-faf2b43b8ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://drive.google.com/uc?export=download&confirm=9iBg&id=1NNHHYIHn55l1P0hd7gm8Mq0oi0ugxlpl\n"
          ]
        }
      ],
      "source": [
        "#Điền link chứa Data\n",
        "url = 'https://drive.google.com/file/d/1NNHHYIHn55l1P0hd7gm8Mq0oi0ugxlpl/view?usp=sharing'\n",
        "id = url.split('/')[-2]\n",
        "\n",
        "#Bỏ qua bước confirm nếu Google yêu cầu diệt virus\n",
        "download_path = 'https://drive.google.com/uc?export=download&confirm=9iBg&id='+ id\n",
        "print(download_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAR6_zvPytwX"
      },
      "source": [
        "**Tải file và giải nén bằng Keras**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "fF7h0cxyTtJk",
        "outputId": "45e98baf-c256-472a-c9a3-d61fe5d85aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://drive.google.com/uc?export=download&confirm=9iBg&id=1NNHHYIHn55l1P0hd7gm8Mq0oi0ugxlpl\n",
            "771194880/771190347 [==============================] - 10s 0us/step\n",
            "771203072/771190347 [==============================] - 10s 0us/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Science&Technology.zip'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keras.utils.get_file(\n",
        "    os.path.join(os.getcwd(), \"Science&Technology.zip\"),\n",
        "    download_path,\n",
        "    extract=True,\n",
        "    archive_format=\"zip\",\n",
        "    cache_dir=\".\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIOVkkK-kMvK",
        "outputId": "7dce5538-a6f0-457d-eb06-46c048fa8acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/datasets/Science&Technology/Output/\n"
          ]
        }
      ],
      "source": [
        "#Đường dẫn chứa thư mục gốc\n",
        "dir = os.getcwd()\n",
        "\n",
        "#Đường dẫn chứa file cần training\n",
        "output_path = dir + \"/datasets/Science&Technology/Output/\"\n",
        "print(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f0mTgdszVvF"
      },
      "source": [
        "###**Preprocess the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "s9mzsjTDkUt1"
      },
      "outputs": [],
      "source": [
        "def Get_Data():\n",
        "    \"\"\"\n",
        "    Function objective: Chia data thành List chứa Audio_feature (Vector của âm thanh), label_audio (Label của âm thanh), text (từ trong âm thanh) từ Folder chứa file training, sau đó gắn vào list\"\n",
        "    Input: Đường dẫn chứa file âm thanh và file text\n",
        "    Ouput: List chứa audio_list (Vector của âm thanh), label_list (Label của âm thanh), text_list (Từ của âm thanh)\n",
        "    \"\"\"\n",
        "    audio_list = []                # Khởi tạo list chứa đường dẫnâm thanh\n",
        "    text_list = []                 # Khởi tạo list chứa text ứng với âm thanh\n",
        "    id_path_list = []              # Khởi tạo list chứa tên story và id audio\n",
        "\n",
        "    story_count = 0                # Biến đếm số lượng story\n",
        "    audio_count = 0                # Biến đếm số lượng file audio\n",
        "    sentense_count = 0             # Biến đếm số lượng câu văn\n",
        "\n",
        "    for story_folder in os.listdir(output_path):\n",
        "      \n",
        "        #Đếm số lượng Story\n",
        "        story_count += 1\n",
        "\n",
        "        #Sắp xếp các file theo thứ tự Alphabet\n",
        "        alphabet = os.listdir(os.path.join(output_path, story_folder))\n",
        "        file_sort = natsort.natsorted(alphabet)\n",
        "\n",
        "        #Duyệt từng file đã sort\n",
        "        for audio_file in file_sort:\n",
        "            \n",
        "            # Đường dẫn âm thanh\n",
        "            audio_path = output_path + story_folder +'/'+ audio_file\n",
        "\n",
        "            # Chỉ đọc file wav\n",
        "            if audio_file.endswith(\"wav\"):\n",
        "\n",
        "                #Đếm số lượng file âm thanh\n",
        "                audio_count += 1\n",
        "\n",
        "                # Thêm đường dẫn của âm thanh vào list\n",
        "                audio_list.append(audio_path)\n",
        "\n",
        "                # Thêm id đường dẫn của âm thanh vào list\n",
        "                id_path_list.append(story_folder + '/' + audio_file)\n",
        "\n",
        "            # Chỉ đọc file text\n",
        "            if audio_file.endswith(\"txt\"):\n",
        "                \n",
        "                #Đọc file text\n",
        "                f =  open(audio_path, \"r\", encoding=\"utf-8\") \n",
        "                content = f.read()\n",
        "\n",
        "                #Đọc từng câu trong file text \n",
        "                sentences = content.split(\"\\n\")\n",
        "\n",
        "                for sentence in sentences:\n",
        "                    \n",
        "                    #Đếm số lượng câu văn\n",
        "                    sentense_count += 1\n",
        "\n",
        "                    #Thêm câu văn vào list\n",
        "                    text_list.append(sentence)\n",
        "            else: \n",
        "                continue\n",
        "    print('Số lượng Story:', story_count)\n",
        "    print('Số lượng Audio:', audio_count)\n",
        "    print('Số lượng Câu:', audio_count)\n",
        "    return audio_list, id_path_list, text_list    #Trả về list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1pXL99u1lsN"
      },
      "source": [
        "**Đọc Data và hiển thị số lượng data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4_4YBXvkXRu",
        "outputId": "47d4303c-05de-4deb-81f4-ab3dad94b1aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng Story: 75\n",
            "Số lượng Audio: 2078\n",
            "Số lượng Câu: 2078\n"
          ]
        }
      ],
      "source": [
        "audio_list, id_path_list, text_list = Get_Data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOcF6BU_1snD"
      },
      "source": [
        "**Hiển thị data trên bảng**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "S2nrPqzXk3eX",
        "outputId": "8035abbb-a0f1-40de-8d6d-fb369eeac251"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e297340c-16e2-427b-bf2a-0877e1647119\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Audio_Path</th>\n",
              "      <th>ID_Path</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/datasets/Science&amp;Technology/Output/Lu...</td>\n",
              "      <td>Lunar_Eclipse_to_Darken_Moon_over_North_South_...</td>\n",
              "      <td>People in many parts of North and South Americ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/datasets/Science&amp;Technology/Output/Lu...</td>\n",
              "      <td>Lunar_Eclipse_to_Darken_Moon_over_North_South_...</td>\n",
              "      <td>A lunar eclipse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/datasets/Science&amp;Technology/Output/Lu...</td>\n",
              "      <td>Lunar_Eclipse_to_Darken_Moon_over_North_South_...</td>\n",
              "      <td>happens when the moon passes through Earth's s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/datasets/Science&amp;Technology/Output/Lu...</td>\n",
              "      <td>Lunar_Eclipse_to_Darken_Moon_over_North_South_...</td>\n",
              "      <td>as our planet goes around the Sun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/datasets/Science&amp;Technology/Output/Lu...</td>\n",
              "      <td>Lunar_Eclipse_to_Darken_Moon_over_North_South_...</td>\n",
              "      <td>This kind of event does not happen every year</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2073</th>\n",
              "      <td>/content/datasets/Science&amp;Technology/Output/Li...</td>\n",
              "      <td>Listening_to_a_Spider_Web/23.wav</td>\n",
              "      <td>There are more than 47,000 kinds of spiders.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2074</th>\n",
              "      <td>/content/datasets/Science&amp;Technology/Output/Li...</td>\n",
              "      <td>Listening_to_a_Spider_Web/24.wav</td>\n",
              "      <td>They all create these webs to provide housing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2075</th>\n",
              "      <td>/content/datasets/Science&amp;Technology/Output/Li...</td>\n",
              "      <td>Listening_to_a_Spider_Web/25.wav</td>\n",
              "      <td>Scientists say silk, the material created to f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2076</th>\n",
              "      <td>/content/datasets/Science&amp;Technology/Output/Li...</td>\n",
              "      <td>Listening_to_a_Spider_Web/26.wav</td>\n",
              "      <td>Buehler said that understanding the living str...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2077</th>\n",
              "      <td>/content/datasets/Science&amp;Technology/Output/Li...</td>\n",
              "      <td>Listening_to_a_Spider_Web/27.wav</td>\n",
              "      <td>could lead to improvements of man made materia...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2078 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e297340c-16e2-427b-bf2a-0877e1647119')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e297340c-16e2-427b-bf2a-0877e1647119 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e297340c-16e2-427b-bf2a-0877e1647119');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             Audio_Path  \\\n",
              "0     /content/datasets/Science&Technology/Output/Lu...   \n",
              "1     /content/datasets/Science&Technology/Output/Lu...   \n",
              "2     /content/datasets/Science&Technology/Output/Lu...   \n",
              "3     /content/datasets/Science&Technology/Output/Lu...   \n",
              "4     /content/datasets/Science&Technology/Output/Lu...   \n",
              "...                                                 ...   \n",
              "2073  /content/datasets/Science&Technology/Output/Li...   \n",
              "2074  /content/datasets/Science&Technology/Output/Li...   \n",
              "2075  /content/datasets/Science&Technology/Output/Li...   \n",
              "2076  /content/datasets/Science&Technology/Output/Li...   \n",
              "2077  /content/datasets/Science&Technology/Output/Li...   \n",
              "\n",
              "                                                ID_Path  \\\n",
              "0     Lunar_Eclipse_to_Darken_Moon_over_North_South_...   \n",
              "1     Lunar_Eclipse_to_Darken_Moon_over_North_South_...   \n",
              "2     Lunar_Eclipse_to_Darken_Moon_over_North_South_...   \n",
              "3     Lunar_Eclipse_to_Darken_Moon_over_North_South_...   \n",
              "4     Lunar_Eclipse_to_Darken_Moon_over_North_South_...   \n",
              "...                                                 ...   \n",
              "2073                   Listening_to_a_Spider_Web/23.wav   \n",
              "2074                   Listening_to_a_Spider_Web/24.wav   \n",
              "2075                   Listening_to_a_Spider_Web/25.wav   \n",
              "2076                   Listening_to_a_Spider_Web/26.wav   \n",
              "2077                   Listening_to_a_Spider_Web/27.wav   \n",
              "\n",
              "                                                   Text  \n",
              "0     People in many parts of North and South Americ...  \n",
              "1                                       A lunar eclipse  \n",
              "2     happens when the moon passes through Earth's s...  \n",
              "3                     as our planet goes around the Sun  \n",
              "4         This kind of event does not happen every year  \n",
              "...                                                 ...  \n",
              "2073       There are more than 47,000 kinds of spiders.  \n",
              "2074  They all create these webs to provide housing ...  \n",
              "2075  Scientists say silk, the material created to f...  \n",
              "2076  Buehler said that understanding the living str...  \n",
              "2077  could lead to improvements of man made materia...  \n",
              "\n",
              "[2078 rows x 3 columns]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Đường dẫn chứa audio\n",
        "df_audio = pd.DataFrame(columns=['Audio_Path'])\n",
        "df_audio['Audio_Path'] = audio_list\n",
        "\n",
        "#Đường dẫn chứa id audio\n",
        "df_path = pd.DataFrame(columns=['ID_Path'])\n",
        "df_path['ID_Path'] = id_path_list\n",
        "\n",
        "#Text ứng với audio\n",
        "df_text = pd.DataFrame(columns=['Text'])\n",
        "df_text['Text'] = text_list\n",
        "\n",
        "#Hiển thị data\n",
        "df_data = pd.concat([df_audio, df_path, df_text, ], axis=1)\n",
        "df_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A25h0Gex2RCU"
      },
      "source": [
        "**Mapping Audio và Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "-yqDZHwwq6b8"
      },
      "outputs": [],
      "source": [
        "def Generate_Data(audio_list, id_path_list, text_list):\n",
        "  \"\"\"\n",
        "  Function objective: Nối âm thanh ứng với text\n",
        "  Input: List chứa đường dẫn file âm thanh, id âm thanh, text của âm thanh\n",
        "  Ouput: Data đã được mapping giữa audio và text\n",
        "  \"\"\"\n",
        "  path_to_text = {}\n",
        "\n",
        "  #Duyệt từng phần tử trong id_path\n",
        "  for i in range(len(id_path_list)):      #Duyệt từng phần tử trong id_path\n",
        "      text = text_list[i]                 #Lấy text tương ứng\n",
        "      id = id_path_list[i]                #Lấy ID từ id_path\n",
        "      path_to_text[id] = text             #Gắn text và id path\n",
        "\n",
        "  data = []\n",
        "  for w in audio_list:                                      #Duyệt từng đường dẫn file audio\n",
        "      id = w.split(\"/\")[-2] + \"/\" + w.split(\"/\")[-1]        #Cắt id từ đường dẫn\n",
        "      if len(path_to_text[id]) < 200:                       #Text ứng với id < hơn 200 từ thì mapping audio và text\n",
        "        data.append({\"audio\": w, \"text\": path_to_text[id]}) #Thêm vào data\n",
        "  return data                                               #Trả về kết quả"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EonmjX2J29L1"
      },
      "source": [
        "**Trả về Data đã được Mapping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yxNQxVfrZgq",
        "outputId": "28f3414f-e1f0-4cb7-b9d5-e4ec75fd5bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data đã được mapping: 2064\n"
          ]
        }
      ],
      "source": [
        "data = Generate_Data(audio_list, id_path_list, text_list)\n",
        "print(\"Data đã được mapping:\", len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNuUGgfG3bet"
      },
      "source": [
        "**Chuẩn hóa data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yw8KdnF9nfI",
        "outputId": "2e12d345-0b9a-487e-e1ef-6b49fe7fd801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số từ vựng: 34\n"
          ]
        }
      ],
      "source": [
        "#Chuyển text tự về dạng vector\n",
        "class VectorizeChar:\n",
        "    def __init__(self, max_len=50):\n",
        "        self.vocab = (\n",
        "            [\"-\", \"#\", \"<\", \">\"]\n",
        "            + [chr(i + 96) for i in range(1, 27)]\n",
        "            + [\" \", \".\", \",\", \"?\"]\n",
        "        )\n",
        "        self.max_len = max_len\n",
        "        self.char_to_idx = {}\n",
        "        for i, ch in enumerate(self.vocab):\n",
        "            self.char_to_idx[ch] = i\n",
        "\n",
        "    def __call__(self, text):\n",
        "        text = text.lower()\n",
        "        text = text[: self.max_len - 2]\n",
        "        text = \"<\" + text + \">\"\n",
        "        pad_len = self.max_len - len(text)\n",
        "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        return self.vocab\n",
        "\n",
        "#Output < 200 kí tự\n",
        "max_target_len = 200  \n",
        "vectorizer = VectorizeChar(max_target_len)\n",
        "print(\"Số từ vựng:\", len(vectorizer.get_vocabulary()))\n",
        "\n",
        "#Khởi tạo data set của text\n",
        "def create_text_ds(data):\n",
        "    \"\"\"\n",
        "    Function objective: Khởi tạo data set chứa text\n",
        "    Input: data đã được mapping. Type(list)\n",
        "    Ouput: List chứa text dạng tensor\n",
        "    \"\"\"\n",
        "    texts = [_[\"text\"] for _ in data]\n",
        "    text_ds = [vectorizer(t) for t in texts]\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
        "    return text_ds\n",
        "\n",
        "\n",
        "def path_to_audio(path):\n",
        "    \"\"\"\n",
        "    Function objective: Lấy vector đặc trưng của audio\n",
        "    Input: Đường dẫn chứa audio. Type(string)\n",
        "    Ouput: Vector đặc trưng của âm thanh. \n",
        "    \"\"\"\n",
        "    #Chuyển audio thành dạng spectrogram sử dụng stft\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
        "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
        "\n",
        "    # Chuẩn hóa audio\n",
        "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
        "    x = tf.math.divide_no_nan(x-means, stddevs)\n",
        "    # x = (x - means) / stddevs\n",
        "    audio_len = tf.shape(x)[0]\n",
        "\n",
        "    # padding audio về 10s\n",
        "    pad_len = 2500\n",
        "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
        "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
        "    return x\n",
        "\n",
        "#Khởi tạo data set của audio\n",
        "def create_audio_ds(data):\n",
        "    \"\"\"\n",
        "    Function objective: Khởi tạo data set chứa vector audio\n",
        "    Input: data đã được mapping. Type(list)\n",
        "    Ouput: Vector đặc trưng của âm thanh dạng tensor \n",
        "    \"\"\"\n",
        "    flist = [_[\"audio\"] for _ in data]\n",
        "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
        "    audio_ds = audio_ds.map(\n",
        "        path_to_audio, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    return audio_ds\n",
        "\n",
        "#Khởi tạo data set của cả audio và text\n",
        "def create_tf_dataset(data, bs=4):\n",
        "    \"\"\"\n",
        "    Function objective: Khởi tạo data set chứa vector audio và text\n",
        "    Input: data đã được mapping - Type(list), batchsize của dữ liệu - type(int)\n",
        "    Ouput: dataset chứa vector audio và text dạng tensor\n",
        "    \"\"\"\n",
        "    audio_ds = create_audio_ds(data)\n",
        "    text_ds = create_text_ds(data)\n",
        "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
        "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
        "    ds = ds.batch(bs)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbjfvFSN56Sg"
      },
      "source": [
        "**Tách data**\n",
        "- Training: **80%**\n",
        "- Evaluation: **20%**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q5DiGvy55gZ",
        "outputId": "a351142f-79da-41b4-c805-bc0f37f53238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 104\n",
            "Evaluation: 26\n"
          ]
        }
      ],
      "source": [
        "#Tách data theo yêu cầu đề bài\n",
        "split = int(len(data) * 0.80)\n",
        "train_data = data[:split]\n",
        "test_data = data[split:]\n",
        "\n",
        "#Gọi lại hàm khởi tại trước đó\n",
        "ds = create_tf_dataset(train_data, bs=16)\n",
        "val_ds = create_tf_dataset(test_data, bs=16)\n",
        "\n",
        "print(\"Train:\", len(ds))\n",
        "print(\"Evaluation:\", len(val_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75LsPfpQxU7C"
      },
      "source": [
        "## **Training the model**\n",
        "Model sử dụng: **Transformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdHdTRPRymTE"
      },
      "source": [
        "###Define the Transformer Input Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "FhIFHkvoqmRy"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(layers.Layer): \n",
        "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
        "        super().__init__()\n",
        "        #Tạo mã thông báo gồm các vectơ có kích thước cố định.\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid) \n",
        "\n",
        "        #Tạo vị trị mã thông báo ( inject thêm thông tin về vị trí của một từ)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid) \n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x)\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n",
        "\n",
        "class SpeechFeatureEmbedding(layers.Layer): \n",
        "    def __init__(self, num_hid=64, maxlen=100):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv2 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv3 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.conv3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdvfMj_YythW"
      },
      "source": [
        "###Transformer Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "bW9NbHQtsC2_"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    \"\"\"\n",
        "    Function objective: Lớp được mã hóa\n",
        "    Input: Layer đã nhúng, số lượng headatention, layer ffd\n",
        "    Ouput: Layer được mã hóa,  đầu vào cho bộ giải mã.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        #Tạo chiếu truy vấn, khóa và giá trị cho Multi-head Attention\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        #Tạo mô hình tuần tự (chuyển danh sách các lớp đến phương thức khởi tạo Tuần tự)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39rAwt_Iyxad"
      },
      "source": [
        "###Transformer Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "W2xWK3bOsFXI"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    \"\"\"\n",
        "    Function objective: Lớp được giải hóa\n",
        "    Input: Layer đã nhúng, số lượng headatention, layer ffd, tỉ lệ dropout\n",
        "    Ouput: Layer được giải mã\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        # Tạo lớp Normalization chuẩn hóa lại đầu ra của multi-head attention\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.self_att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        #Dropout để các unit bị ẩn đi trong quá trình training làm giảm tính dự đoán của model\n",
        "        self.self_dropout = layers.Dropout(0.5)\n",
        "        self.enc_dropout = layers.Dropout(0.1)\n",
        "        self.ffn_dropout = layers.Dropout(0.1)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "        \"\"\"Khi Decoder dịch đến từ thứ i, phần sau sẽ bị che lại (masked) và \n",
        "        Decoder chỉ được phép nhìn thấy phần nó đã dịch trước đó.\n",
        "        Để ngăn chặn luồng thông tin từ mã thông báo trong tương lai đến mã thông báo hiện tại\n",
        "        \"\"\"\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j - n_src + n_dest\n",
        "        mask = tf.cast(m, dtype)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, enc_out, target):\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
        "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
        "        enc_out = self.enc_att(target_norm, enc_out)\n",
        "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
        "        ffn_out = self.ffn(enc_out_norm)\n",
        "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
        "        return ffn_out_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cguEGwmwy3nk"
      },
      "source": [
        "###Complete the Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "1NZDCsw8sHzV"
      },
      "outputs": [],
      "source": [
        "class Transformer(keras.Model):\n",
        "    \"\"\"\n",
        "    Function objective: Tạo mô hình Transformer\n",
        "    Input: Hidden layer, head-attention, feed_forward, số lượng từ source, số lượng từ đích, layer mã hóa và giải mả, num class.\n",
        "    Ouput: Layer được giải mã\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hid=64,\n",
        "        num_head=2,\n",
        "        num_feed_forward=128,\n",
        "        source_maxlen=100,\n",
        "        target_maxlen=100,\n",
        "        num_layers_enc=4,\n",
        "        num_layers_dec=1,\n",
        "        num_classes=10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "        self.num_layers_enc = num_layers_enc\n",
        "        self.num_layers_dec = num_layers_dec\n",
        "        self.target_maxlen = target_maxlen\n",
        "        self.num_classes = num_classes\n",
        "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
        "        self.dec_input = TokenEmbedding(\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
        "        )\n",
        "        self.encoder = keras.Sequential(\n",
        "            [self.enc_input]\n",
        "            + [\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
        "                for _ in range(num_layers_enc)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for i in range(num_layers_dec):\n",
        "            setattr(\n",
        "                self,\n",
        "                f\"dec_layer_{i}\",\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
        "            )\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "    def decode(self, enc_out, target):\n",
        "        y = self.dec_input(target)\n",
        "        for i in range(self.num_layers_dec):\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
        "        return y\n",
        "\n",
        "    def call(self, inputs):\n",
        "        source = inputs[0]\n",
        "        target = inputs[1]\n",
        "        x = self.encoder(source)\n",
        "        y = self.decode(x, target)\n",
        "        return self.classifier(y)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self([source, dec_input])\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        preds = self([source, dec_input])\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def generate(self, source, target_start_token_idx):\n",
        "        \"\"\"đầu vào sử dụng greedy decoding.\"\"\"\n",
        "        bs = tf.shape(source)[0]\n",
        "        enc = self.encoder(source)\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "        dec_logits = []\n",
        "        for i in range(self.target_maxlen - 1):\n",
        "            dec_out = self.decode(enc, dec_input)\n",
        "            logits = self.classifier(dec_out)\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
        "            dec_logits.append(last_logit)\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "        return dec_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reQ9o8Drztad"
      },
      "source": [
        " ### Callbacks to display predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "qULXqYmvrzvj"
      },
      "outputs": [],
      "source": [
        "class DisplayOutputs(keras.callbacks.Callback):\n",
        "    def __init__( #Tạo các lệnh gọi lại\n",
        "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
        "    ):\n",
        "        \"\"\"Hiển thị kết quả đầu ra sau mỗi epoch\n",
        "\n",
        "        Args:\n",
        "             batch: test chứa các khóa \"nguồn\" và \"đích\"\n",
        "             idx_to_token: Danh sách chứa các mã thông báo từ vựng tương ứng với các chỉ số của chúng\n",
        "             target_start_token_idx: Chỉ mục mã thông báo bắt đầu trong từ vựng đích\n",
        "             target_end_token_idx: Chỉ mục mã thông báo kết thúc trong từ vựng đích\n",
        "        \"\"\"\n",
        "        self.batch = batch\n",
        "        self.target_start_token_idx = target_start_token_idx\n",
        "        self.target_end_token_idx = target_end_token_idx\n",
        "        self.idx_to_char = idx_to_token\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 5 != 0:\n",
        "            return\n",
        "        source = self.batch[\"source\"]\n",
        "        target = self.batch[\"target\"].numpy()\n",
        "        bs = tf.shape(source)[0]\n",
        "        preds = self.model.generate(source, self.target_start_token_idx)\n",
        "        preds = preds.numpy()\n",
        "        for i in range(bs):\n",
        "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
        "            prediction = \"\"\n",
        "            for idx in preds[i, :]:\n",
        "                prediction += self.idx_to_char[idx]\n",
        "                if idx == self.target_end_token_idx:\n",
        "                    break\n",
        "            wer_score = wer(target_text, prediction)    #Tính toán WER\n",
        "            print(\"-\" * 100)\n",
        "            print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "            print(\"-\" * 100)\n",
        "            print(f\"target:     {target_text.replace('-','')}\")\n",
        "            print(f\"prediction: {prediction}\\n\")\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O5oBFdkz1oY"
      },
      "source": [
        "### Learning rate schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "hLH9DZVrr2Md"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        init_lr=0.00001,\n",
        "        lr_after_warmup=0.001,\n",
        "        final_lr=0.00001,\n",
        "        warmup_epochs=15,\n",
        "        decay_epochs=85,\n",
        "        steps_per_epoch=203,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.init_lr = init_lr\n",
        "        self.lr_after_warmup = lr_after_warmup\n",
        "        self.final_lr = final_lr\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.decay_epochs = decay_epochs\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def calculate_lr(self, epoch):\n",
        "        \"\"\" linear warm up - linear decay \"\"\"\n",
        "        warmup_lr = (\n",
        "            self.init_lr\n",
        "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
        "        )\n",
        "        decay_lr = tf.math.maximum(\n",
        "            self.final_lr,\n",
        "            self.lr_after_warmup\n",
        "            - (epoch - self.warmup_epochs)\n",
        "            * (self.lr_after_warmup - self.final_lr)\n",
        "            / (self.decay_epochs),\n",
        "        )\n",
        "        return tf.math.minimum(warmup_lr, decay_lr)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        epoch = step // self.steps_per_epoch\n",
        "        return self.calculate_lr(epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "b4nDu2Mtr6YP"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(val_ds))\n",
        "\n",
        "# Tạo từ vựng để chuyển đổi các chỉ số dự đoán thành các ký tự\n",
        "idx_to_char = vectorizer.get_vocabulary()\n",
        "display_cb = DisplayOutputs(\n",
        "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",
        ")  # đặt các đối số trong '<' và '>'\n",
        "\n",
        "#Khởi tạo mô hình\n",
        "model = Transformer(\n",
        "    num_hid=128,\n",
        "    num_head=2,\n",
        "    num_feed_forward=128,\n",
        "    target_maxlen=200,\n",
        "    num_layers_enc=4,\n",
        "    num_layers_dec=1,\n",
        "    num_classes=34,\n",
        ")\n",
        "\n",
        "#Hàm mất mát\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, label_smoothing=0.1,\n",
        ")\n",
        "\n",
        "#Khởi tạo learning rate\n",
        "learning_rate = CustomSchedule(\n",
        "    init_lr=0.00001,\n",
        "    lr_after_warmup=0.001,\n",
        "    final_lr=0.00001,\n",
        "    warmup_epochs=15,\n",
        "    decay_epochs=85,\n",
        "    steps_per_epoch=len(ds),\n",
        ")\n",
        "\n",
        "#Khởi tạo hàm tối ưu\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "#Compile Mô hình\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXfraZQfx6jB"
      },
      "source": [
        "## **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOOKev_k9mUA",
        "outputId": "b670686e-fe25-4597-cc51-0ca9199d038c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "104/104 [==============================] - ETA: 0s - loss: 1.4104----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.5000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <until now, huge ichthyosaurs had not been known to have lived at the end of the triassic period>\n",
            "prediction: <iaoncrererererereexer g#a   tatns xeq taeq sst   tstntg o o tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 2.0769\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <scientists had thought the group disappeared from earth a few million years earlier>\n",
            "prediction: <iaoncrererererereexer g#e   tatns xeq taeq sst   tstntg o o tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.8667\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <the blue whale has long been considered the largest animal to have lived on earth>\n",
            "prediction: <iaoncx nsrererereexer g#e   tatns xeq taeq sst   tstntg o o tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.8571\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <it can grow to ## meters long and weigh close to ### metric tons>\n",
            "prediction: <iaoncrererererereexer g#a   tatns xeq taeq sst   tstntg oie tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.4211\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <but study writer sander said more research on the triassic ichthyosaur may renew questions about the planet#s largest creatures>\n",
            "prediction: <iaoncrererererereexer g#e   tatns xeq taeq sst   tstntg o o tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 8.6667\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <i#m jill robbins>\n",
            "prediction: <iaoncxinsrererereexer g#e   tatns xeq taeq sst   tstntg oie tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.8000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <in the early ####s, scientist sue hendrickson discovered the world#s largest tyrannosaurus rex dinosaur skeleton>\n",
            "prediction: <iaoncrererererereexer g#a   tatns xeq taeq sst   tstntg o o tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 2.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <it was found in rock formations in the american state of south dakota>\n",
            "prediction: <iaoncrererererereexer g#a   tatns xeq taeq sst   tstntg oie tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 4.3333\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <the skeleton is known as sue>\n",
            "prediction: <iaoncereq atsistastsv g#e   tatns xeq taeq sst   tstntg oie tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 6.7500\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <in honor of hendrickson>\n",
            "prediction: <iaoncereq atsistastsv g#e   tatns xeq taeq sst   tstntg oie tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.5000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <visitors to the field museum of natural history in chicago have been able to see sue since ####>\n",
            "prediction: <iaoncrererererereexer g#a   tatns xeq taeq sst   tstntg o o tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 5.4000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <however, for most of ####>\n",
            "prediction: <iaoncereq atsrereexer g#e   tatns xeq taeq sst   tstntg oie tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 4.5000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <sue was nowhere to be seen>\n",
            "prediction: <iaoncereq atsistastsv g#e   tatns xeq taeq sst   tstntg oie tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 2.0769\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <instead, experts spent ## months working on and adding to the worldfamous skeleton>\n",
            "prediction: <iaoncrererererereexer g#a   tatns xeq taeq sst   tstntg o o tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 3.0000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <jaap hoogstraten is the field museum#s director of exhibitions>\n",
            "prediction: <iaoncx nsrererereexer g#e   tatns xeq taeq sst   tstntg oie tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Word Error Rate: 1.8571\n",
            "----------------------------------------------------------------------------------------------------\n",
            "target:     <he says museum workers have learned new things about the skeleton#s shape since ####>\n",
            "prediction: <iaoncrererererereexer g#a   tatns xeq taeq sst   tstntg oie tasru g nde g o  oi g httcd nas o  heerd  g as ttieeud  geerersna tgpsreeree g g>\n",
            "\n",
            "104/104 [==============================] - 240s 2s/step - loss: 1.4104 - val_loss: 1.3049\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spXjWWUpBNpX",
        "outputId": "48e76d3c-74dd-426d-a825-96983e9be7c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " speech_feature_embedding_6   (None, None, 128)        542464    \n",
            " (SpeechFeatureEmbedding)                                        \n",
            "                                                                 \n",
            " token_embedding_6 (TokenEmb  multiple                 29952     \n",
            " edding)                                                         \n",
            "                                                                 \n",
            " sequential_49 (Sequential)  (None, None, 128)         1204480   \n",
            "                                                                 \n",
            " transformer_decoder_15 (Tra  multiple                 297728    \n",
            " nsformerDecoder)                                                \n",
            "                                                                 \n",
            " dense_94 (Dense)            multiple                  4386      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,536,548\n",
            "Trainable params: 1,536,546\n",
            "Non-trainable params: 2\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HooAij3x90p"
      },
      "source": [
        "## **Demonstration**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "51802077_51800366_51800098_Final_Report.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
